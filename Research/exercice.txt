Il a deux types de "neuronnes" pour le moment, les perceptrons et les sigmoids. Pour
comprendre la difference il faut s'interesser a ce que contiennent les dit neuronnes.
Un neuronne contient un calcul, une somme plus exactement. Le but de cette somme est d'appliquer
un calcul a different input et de ressortir un output simple. Les inputs vont avoir un poids (w) et 
a cette somme d input ponderee on va venir y ajouter un biais qui est l inverse d'un seuil.
Et c'est donc ici que la difference entre nos neuronnes se fait. Le perceptron peut etre definit par
output =    | 0 si sum(wj.xj) + b <= 0 
            | 1 si sum(wj.x) + b > 0
alors que le sigmoid:
output = 1 / (1 + e^-z) sachant que z = sum(wj.xj) + b

Ex1:
Montrons que le comportement du network ne change pas si nous multiplions les weights ainsi que les biais d'un perceptron:

output =    | 0 si w.x + b <= 0 
            | 1 si w.x + b > 0

si nous multiplions le tout:

output =    | 0 si cw.x + cb <= 0
            | 1 si cw.x + cb > 0 
Sachant que w.x + b = (cw.x + cb) / c et que 0 / c = 0 alors le comportement du network reste inchange.

Ex2:
meme chose mais pour un sigmoid:

output = sig(z) = 1 / (1 + e^-z) soit 1 / (1 + e^-(w.x + b)) pour sig(w.x + b)

Sachant que w.x + b != 0
Si w.x + b > 0 alors output de sig(w.x + b) = 1
Si w.x + b < 0 alors output sig(w.x + b) = 0
Alors si on a sig(cw.x + cb) sachant que c->inf alors meme regle que precedement,
Si cw.x + cb > 0 alors output de sig(cw.x + cb) -> 1
Si cw.x + cb < 0 alors output sig(cw.x + cb) -> 0
Puisque tous les sigmoids d un reseau appliquent cette regle alors le comportement du network reste inchange
mnt dans le cas ou w.x + b = 0 alors cw.x + cb = 0 aussi et peu importe si c->inf sig(cw.x + cb) = 0.5 donc le comportement d un perceptron ne sera jamais atteint peu importe la valeur de c.

Quadratic Cost Function ->
C(w,b)≡(1/2n)∑x(∥y(x)−a∥2).
w = all weights
b = all biaises
n = number of inputs
a = vector of outputs
x = all inputs
y(x) = vector of expected output for an input x

La Quadratic Cost Function ou MSE(Mean Squerred Error) est une fonction qui calcule l'erreur
de notre algo en comparant ses outputs par rapport a ceux attendus. Donc plus a sera proche
de y(x) plus C(w,b) tendra vers 0 la ou a l'inverse il tendra vers 1.

Ex3: Ici le but de l'exercice va etre de calculer les weights et biaises pour que notre
le output layer ne comporte plus que 4 output neurons. 
D'apres l'ecriture binaire des chiffres on sait que le premier output neuron (on ecrit ici de droite a gauche 1 = 0001)
va output 1 lorsque l input est 8 ou 9 et le reste du temps ca sera 0. Donc on sait que le weight pour l input de 8 et 9
est de 1 et 0 pour les autres chiffres. Tel que w(0,0,0,0,0,0,0,0,1,1).x avec x vecteur des outputs de la layer precedente.
Sachant que la precision des outputs de la layer d avant est de >= 0.99 lors de l activation et <= 0.01 le reste du temps.
Donc on sait que :  0.99 <= σ(w.x + b) pour 8 et 9, et σ(w.x + b) <= 0.01 pour les autres.
Plus simplement on cherche b afin que σ(b) ~ 0 et σ(1 + b) ~ 1;
Donc on peut set b = -0.5 puisqu il remplit les deux conditions,
On se rappelle que on cherche w tel que:
    0.99 <= σ(w(0.99 + b)) et σ(w(0.02 + b)) <= 0.01

Plus globalement on peut dire qu on a une precision α = 0.01 donc 
    | 1 - α <= σ(w(1 - α + b))
    | σ(w(2α + b)) <= α

Donc d apres la definition au dessus d un sigmoid σ cela nous donne:
    | w >= (ln((1-α)/α))/(1-α+b)
    | w >= -(ln((1-α)/α))/(2α+b)

    | w >= 9.38
    | w >= 9.58

Maintenant on va generaliser tout ca pour, pas besoin de conclure pour cet unique neuron.
Un neuron prendra l output de n <= 10 neurons de la layer precedente donc on a :
    1 - α <= w.x <= 1 + (n - 1)α s'il doit s activer et 0 <= w.x <= nα pour les autres

Donc nos conditions deviennent:
    | 1 - α <=  σ(w(1-α+b))
    | σ(w(nα+b)) <= α

avec nα < 0.5 et donc nα + b < 0
    | w >= (ln((1-α)/α))/(1-α+b)
    | w >= -(ln((1-α)/α))/(nα+b)

donc pour chaque neuron avec n <= 10
    | w >= 9.38
    | w >= 11.49

donc globalement w >= 12

On aura donc un vecteur final remplit de 0 et de 12, on peut dont en deduire que maintenant b = -6
pusque pour passer de 1 a 12 on a applique un facteur 12 au weight alors on fait pareil pour le biais.

Neuron 1: w=12(0,0,0,0,0,0,0,0,1,1); b=−6
Neuron 2: w=12(0,0,0,0,1,1,1,1,0,0); b=−6
Neuron 3: w=12(0,0,1,1,0,0,1,1,0,0); b=−6
Neuron 4: w=12(0,1,0,1,0,1,0,1,0,1); b=−6

Ex4:
Ex5:

CODE EXPLAINATION:

backprop(x,y) -> x c'est les inputs et y les outputs attendus. Ca peut etre un vector ou un nombre.

nabla_b = [np.zeros(b.shape) for b in self.biases] -> le but va etre de shape nabla_b comme le vector de biases.
nabla_w = [np.zeros(w.shape) for w in self.weights] -> Pareil pour weights

Apres avoir developpe un algo de training simple pour reconnaitre des handwritten digits, la question de l'implementation se pose
par rapport au pong. Il semble que le modele utilise pour les nombres est loin d'etre le plus optimise pour une IA de jeu.
Il va dont falloir trouver une alternative. Celle ci s'appelle le deep Q learning ou Q learning en fonction du degres de 
complexite. Le deep Q learning utilise un autre network pour creer des inputs a un network target alors que le second prend 
directement des inputs fixs. Dans tous les cas ce type d algo a un entrainement bien different du precedent puisque
desormais l algo cherche a collecter un maximum de reward.

Implementation of a Q-learning algorithm based in C++ in the folder Q-learning;
The Q-learning is a machine learning algorithm taking part in the renforcement learning subregion.
It is way differente from deep learning but it's very interesting. To give a quick explaination the algo can be
devided in two section: The agent and the environment. The agent can interact with the environment to discover it
and earn some potential reward. In my implementation the environment is 4x4 map with a start (S) top left and a goal (G) bottom right.
There is some frozen ice (F) that the agent can walk on, and some hole (H) where it could fall. 

Exemple:
    'S'FFF
    HFFH
    FHFF
    FHFG

Now we have to create a QMatrix for each state (in our case the emplacement of our agent) and depending of our possible actions. In this exemple
there is 4 actions (up, down, right, left). So our QMatrix is going to store our QValue for each pair {state:action};

Exemple:
            |Up    |Down  |Right |Left
    State 0:|0     |0     |0     |0
    State 1:|0     |0     |0     |0
    State...

The Qvalue is the average got reward at the end of a lap for each action took at a state. If the QValue is 0 that signify that the lap will stop here
if the action is taken (like the agent fall in to a hole). The purpose of our training is to define all the Qvalues so at the end the agent should just
select the highest value per state to have the best pathing to the goal.